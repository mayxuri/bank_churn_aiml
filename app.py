"""
Bank Customer Churn Prediction - Streamlit Application
Professional 5-page dashboard for churn prediction and analysis

Pages:
1. Home - Overview and business context
2. Predict Churn - Interactive prediction interface
3. Data Analytics - EDA, clustering, and association rules
4. Model Performance - Comprehensive model comparison
5. Batch Predictions - Upload CSV and retention strategy

Author: ML Lab Assignment
Date: 2025
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import joblib
import json
from pathlib import Path
import tensorflow as tf
from sklearn.metrics import confusion_matrix, roc_curve, auc
import time

# Custom utilities
from utils import (
    get_risk_category,
    format_currency,
    calculate_business_cost,
    calculate_clv,
    get_retention_recommendations,
    validate_input_data,
    interpret_cluster
)

# Page configuration
st.set_page_config(
    page_title="Bank Churn Prediction System",
    # page_icon="üè¶",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for professional banking theme
st.markdown("""
<style>
    .main {
        background-color: #f5f7fa;
    }
    h1 {
        color: #1e3a8a;
        font-family: 'Arial', sans-serif;
    }
    h2 {
        color: #2563eb;
    }
    h3 {
        color: #3b82f6;
    }
    h4{
        color: #000000;
    }
    .stMetric {
        background-color: white;
        padding: 15px;
        border-radius: 10px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    .risk-high {
        background-color: #fee2e2;
        color: #991b1b;
        padding: 15px;
        border-radius: 8px;
        font-weight: bold;
        font-size: 18px;
        text-align: center;
        border: 2px solid #dc2626;
    }
    .risk-medium {
        background-color: #fef3c7;
        color: #92400e;
        padding: 15px;
        border-radius: 8px;
        font-weight: bold;
        font-size: 18px;
        text-align: center;
        border: 2px solid #f59e0b;
    }
    .risk-low {
        background-color: #d1fae5;
        color: #065f46;
        padding: 15px;
        border-radius: 8px;
        font-weight: bold;
        font-size: 18px;
        text-align: center;
        border: 2px solid #10b981;
    }
    .prediction-box {
        background-color: white;
        padding: 20px;
        border-radius: 12px;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        margin: 10px 0;
    }
    .recommendation-box {
        background-color: #eff6ff;
        padding: 15px;
        border-radius: 8px;
        border-left: 4px solid #3b82f6;
        margin: 10px 0;
    }
    .sidebar .sidebar-content {
        background-color: #1e293b;
    }
</style>
""", unsafe_allow_html=True)


# Cache model loading
@st.cache_resource
def load_models():
    """Load all trained models and preprocessing objects."""
    models = {}

    model_files = {
        'Logistic Regression': 'models/logistic_regression.pkl',
        'Decision Tree': 'models/decision_tree.pkl',
        'Random Forest': 'models/random_forest.pkl',
        'Random Forest Optimized': 'models/random_forest_optimized.pkl',
        'XGBoost': 'models/xgboost.pkl',
        'XGBoost Optimized': 'models/xgboost_optimized.pkl',
        'SVM': 'models/svm.pkl',
        'Gradient Boosting': 'models/gradient_boosting.pkl'
    }

    # Load traditional models
    for name, path in model_files.items():
        if Path(path).exists():
            models[name] = joblib.load(path)

    # Load neural network
    if Path('models/neural_network.h5').exists():
        models['Neural Network'] = tf.keras.models.load_model('models/neural_network.h5')

    # Load preprocessing objects
    scaler = joblib.load('models/scaler.pkl')
    label_encoder_gender = joblib.load('models/label_encoder_gender.pkl')
    feature_names = joblib.load('models/feature_names.pkl')

    return models, scaler, label_encoder_gender, feature_names


@st.cache_data
def load_data():
    """Load the dataset."""
    return pd.read_csv('data/Churn_Modelling.csv')


@st.cache_data
def load_results():
    """Load model results and metrics."""
    if Path('results/results_summary.json').exists():
        with open('results/results_summary.json', 'r') as f:
            return json.load(f)
    return None


@st.cache_data
def load_cluster_profiles():
    """Load customer segmentation cluster profiles."""
    if Path('results/cluster_profiles.csv').exists():
        return pd.read_csv('results/cluster_profiles.csv')
    return None


@st.cache_data
def load_association_rules():
    """Load association rules."""
    if Path('results/association_rules.csv').exists():
        return pd.read_csv('results/association_rules.csv')
    return None


def preprocess_input(input_data, scaler, label_encoder_gender, feature_names):
    """
    Preprocess user input for prediction.

    Args:
        input_data: Dictionary of input features
        scaler: Fitted StandardScaler
        label_encoder_gender: Fitted LabelEncoder for gender
        feature_names: List of feature names from training

    Returns:
        Preprocessed numpy array ready for prediction
    """
    # Create dataframe
    df = pd.DataFrame([input_data])

    # Feature engineering
    df['BalanceSalaryRatio'] = df['Balance'] / (df['EstimatedSalary'] + 1)
    df['TenureAgeRatio'] = df['Tenure'] / (df['Age'] + 1)
    df['BalancePerProduct'] = df['Balance'] / (df['NumOfProducts'] + 1)

    # Age categories
    if df['Age'].iloc[0] < 35:
        df['AgeGroup_Middle'] = 0
        df['AgeGroup_Senior'] = 0
    elif df['Age'].iloc[0] < 50:
        df['AgeGroup_Middle'] = 1
        df['AgeGroup_Senior'] = 0
    else:
        df['AgeGroup_Middle'] = 0
        df['AgeGroup_Senior'] = 1

    # Balance categories
    balance = df['Balance'].iloc[0]
    if balance < 50000:
        df['BalanceCategory_Medium'] = 0
        df['BalanceCategory_High'] = 0
    elif balance < 100000:
        df['BalanceCategory_Medium'] = 1
        df['BalanceCategory_High'] = 0
    else:
        df['BalanceCategory_Medium'] = 0
        df['BalanceCategory_High'] = 1

    # Credit Score categories
    credit_score = df['CreditScore'].iloc[0]
    if credit_score < 600:
        df['CreditScoreCategory_Fair'] = 0
        df['CreditScoreCategory_Good'] = 0
    elif credit_score < 700:
        df['CreditScoreCategory_Fair'] = 1
        df['CreditScoreCategory_Good'] = 0
    else:
        df['CreditScoreCategory_Fair'] = 0
        df['CreditScoreCategory_Good'] = 1

    # Encode gender
    df['Gender'] = label_encoder_gender.transform([input_data['Gender']])[0]

    # One-hot encode geography
    df['Geography_France'] = 1 if input_data['Geography'] == 'France' else 0
    df['Geography_Germany'] = 1 if input_data['Geography'] == 'Germany' else 0
    df['Geography_Spain'] = 1 if input_data['Geography'] == 'Spain' else 0

    # Select only the features used in training
    df_processed = df[feature_names]

    # Scale features
    X_scaled = scaler.transform(df_processed)

    return X_scaled


def predict_churn(model, X, model_name):
    """
    Make prediction using selected model.

    Args:
        model: Trained model
        X: Preprocessed features
        model_name: Name of the model

    Returns:
        Tuple of (prediction, probability)
    """
    if model_name == 'Neural Network':
        probability = model.predict(X, verbose=0)[0][0]
        prediction = 1 if probability > 0.5 else 0
    else:
        prediction = model.predict(X)[0]
        probability = model.predict_proba(X)[0][1]

    return prediction, probability


def page_home():
    """Page 1: Home - Overview and business context"""
    st.title("Bank Customer Churn Prediction System")
    st.markdown("### Advanced ML System for Predicting and Preventing Customer Attrition")

    st.markdown("---")

    # Introduction
    col1, col2 = st.columns([2, 1])

    with col1:
        st.markdown("""
        ### Business Problem

        Customer churn is one of the most critical challenges facing the banking industry.
        When customers close their accounts and move to competitors, banks lose:

        - **Revenue Stream**: Average customer lifetime value of $1,500
        - **Acquisition Investment**: Initial $200 spent on customer acquisition
        - **Growth Opportunity**: Potential for cross-selling additional products
        - **Market Share**: Competitive advantage in the financial services sector

        This ML-powered system predicts which customers are likely to churn, enabling
        proactive retention strategies that are 10x more cost-effective than acquiring new customers.

        ### Solution Approach

        Our comprehensive system leverages:
        - **7 Machine Learning Models**: Including Neural Networks and ensemble methods
        - **Customer Segmentation**: K-Means clustering for targeted strategies
        - **Pattern Discovery**: Association rule mining to identify churn triggers
        - **Real-time Predictions**: Instant risk assessment for any customer
        - **Batch Processing**: Analyze entire customer database at once
        """)

    with col2:
        st.markdown("### Dataset Overview")

        # Load data for metrics
        try:
            df = load_data()
            churn_rate = df['Exited'].mean() * 100
            total_customers = len(df)
            num_features = 13
            num_models = 7

            st.metric("Total Customers", f"{total_customers:,}")
            st.metric("Churn Rate", f"{churn_rate:.1f}%", delta=f"{churn_rate-20:.1f}% vs target")
            st.metric("Features Analyzed", num_features)
            st.metric("ML Models Trained", num_models)
        except:
            st.warning("Dataset not loaded. Please run train_models.py first.")

    st.markdown("---")

    # Cost visualization
    st.markdown("### Financial Impact of Churn")

    col1, col2, col3 = st.columns(3)

    with col1:
        st.markdown("""
        <div class="prediction-box">
            <h4>Customer Acquisition</h4>
            <h2 style="color: #dc2626;">$200</h2>
            <p>Cost to acquire one new customer through marketing and onboarding</p>
        </div>
        """, unsafe_allow_html=True)

    with col2:
        st.markdown("""
        <div class="prediction-box">
            <h4>Lifetime Value</h4>
            <h2 style="color: #16a34a;">$1,500</h2>
            <p>Average revenue generated per customer over their lifetime</p>
        </div>
        """, unsafe_allow_html=True)

    with col3:
        st.markdown("""
        <div class="prediction-box">
            <h4>Retention Campaign</h4>
            <h2 style="color: #3b82f6;">$50</h2>
            <p>Cost to execute targeted retention strategy per customer</p>
        </div>
        """, unsafe_allow_html=True)

    # ROI calculation
    st.markdown("### Return on Investment")

    cost_data = pd.DataFrame({
        'Scenario': ['Lose Customer (No Action)', 'Retention Campaign (Proactive)', 'New Acquisition (Reactive)'],
        'Cost': [1500, 50, 200],
        'Description': [
            'Lost lifetime value',
            'Retention offer cost',
            'New customer acquisition'
        ]
    })

    fig = px.bar(
        cost_data,
        x='Scenario',
        y='Cost',
        title='Cost Comparison: Retention vs Acquisition',
        labels={'Cost': 'Cost ($)'},
        color='Cost',
        color_continuous_scale='RdYlGn_r',
        text='Cost'
    )
    fig.update_traces(texttemplate='$%{text}', textposition='outside')
    fig.update_layout(height=400, showlegend=False)
    st.plotly_chart(fig, use_container_width=True)

    st.success("""
    **Key Insight**: Investing $50 in retention campaigns can save $1,450 per customer
    (cost of churn minus retention cost), representing a 2,900% ROI!
    """)

    st.markdown("---")

    # Course Outcomes
    st.markdown("### Technical Implementation - 5 Course Outcomes (COs)")

    with st.expander("CO1: AI-based Heuristic Techniques"):
        st.markdown("""
        **Grid Search CV for Hyperparameter Optimization**

        - Systematically tested combinations of hyperparameters for Random Forest and XGBoost
        - Used 5-fold cross-validation to ensure robust optimization
        - Prioritized Recall metric (catching churners) over accuracy
        - Achieved measurable performance improvements in both models

        *Business Justification*: Even 1% improvement in churn prediction translates to
        millions in retained revenue. Grid Search ensures we find the optimal model configuration.
        """)

    with st.expander("CO2: Data Preprocessing"):
        st.markdown("""
        **Comprehensive Data Preparation Pipeline**

        - Feature Scaling: StandardScaler for numerical features
        - Encoding: LabelEncoder (Gender), OneHotEncoder (Geography)
        - Class Imbalance: SMOTE to balance 20% churn rate
        - Feature Engineering: Created 6 new features (BalanceSalaryRatio, AgeGroup, etc.)

        *Business Justification*: Raw banking data requires careful preprocessing.
        Feature engineering creates meaningful business metrics that improve model accuracy.
        """)

    with st.expander("CO3: Supervised Learning - Classification"):
        st.markdown("""
        **6 Classification Models + Neural Network**

        Models: Logistic Regression, Decision Tree, Random Forest, XGBoost, SVM, Gradient Boosting

        Evaluation Metrics:
        - Accuracy, Precision, Recall (most important), F1-Score, ROC-AUC
        - Confusion Matrix analysis
        - Business cost calculation

        *Business Justification*: Different models capture different patterns.
        Ensemble methods like Random Forest and XGBoost excel at tabular banking data.
        """)

    with st.expander("CO4: Unsupervised Learning"):
        st.markdown("""
        **A) K-Means Clustering (Customer Segmentation)**

        - Segmented customers into 4 distinct groups based on behavior
        - Identified high-value clusters and at-risk segments
        - Created targeted retention strategies for each segment

        **B) Association Rule Mining (Pattern Discovery)**

        - Applied Apriori algorithm to discover churn patterns
        - Found rules with 70%+ confidence predicting churn
        - Example: "IF Germany + Single Product + Inactive THEN 78% churn"

        *Business Justification*: Segmentation enables personalized marketing.
        Pattern discovery reveals hidden churn triggers for proactive intervention.
        """)

    with st.expander("CO5: Neural Networks"):
        st.markdown("""
        **Deep Learning with TensorFlow/Keras**

        Architecture:
        - Input Layer: 20+ features
        - Hidden Layer 1: 128 neurons (ReLU) + BatchNorm + Dropout(0.3)
        - Hidden Layer 2: 64 neurons (ReLU) + BatchNorm + Dropout(0.3)
        - Hidden Layer 3: 32 neurons (ReLU) + BatchNorm + Dropout(0.2)
        - Output Layer: 1 neuron (Sigmoid)

        Training: 100 epochs with EarlyStopping (patience=15)

        *Business Justification*: Neural networks capture complex non-linear interactions
        between features that traditional models miss, improving prediction accuracy.
        """)

    st.markdown("---")

    # Navigation guide
    st.markdown("### Get Started")

    st.info("""
    **Next Steps:**

    1. **Predict Churn**: Use the prediction tool to assess individual customer risk
    2. **Data Analytics**: Explore customer segments and churn patterns
    3. **Model Performance**: Compare all ML models and their business impact
    4. **Batch Predictions**: Upload your customer database for mass analysis

    Use the sidebar navigation to explore different sections of the application.
    """)


def page_predict():
    """Page 2: Predict Customer Churn - Interactive prediction interface"""
    st.title("Predict Customer Churn Risk")
    st.markdown("### Enter customer information to assess churn probability and get retention recommendations")

    st.markdown("---")

    # Load models
    try:
        models, scaler, label_encoder_gender, feature_names = load_models()
    except Exception as e:
        st.error(f"Error loading models: {str(e)}")
        st.warning("Please run train_models.py first to train and save the models.")
        return

    # Input form
    st.markdown("### Customer Information")

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("#### Personal Information")

        customer_id = st.text_input("Customer ID (Optional)", "CUST001", help="For display purposes only")
        surname = st.text_input("Surname (Optional)", "Smith", help="For display purposes only")

        geography = st.selectbox(
            "Geography",
            options=['France', 'Spain', 'Germany'],
            help="Country where customer resides"
        )

        gender = st.selectbox(
            "Gender",
            options=['Male', 'Female']
        )

        age = st.slider(
            "Age",
            min_value=18,
            max_value=92,
            value=38,
            help="Customer age in years"
        )

        credit_score = st.slider(
            "Credit Score",
            min_value=350,
            max_value=850,
            value=650,
            help="Numerical credit score (higher is better)"
        )

    with col2:
        st.markdown("#### Banking Details")

        tenure = st.slider(
            "Tenure (Years)",
            min_value=0,
            max_value=10,
            value=5,
            help="Number of years with the bank"
        )

        balance = st.number_input(
            "Account Balance ($)",
            min_value=0.0,
            max_value=250000.0,
            value=75000.0,
            step=1000.0,
            help="Current account balance"
        )

        num_products = st.slider(
            "Number of Products",
            min_value=1,
            max_value=4,
            value=1,
            help="Number of bank products used"
        )

        has_credit_card = st.checkbox(
            "Has Credit Card",
            value=True
        )

        is_active_member = st.checkbox(
            "Is Active Member",
            value=True,
            help="Active engagement with bank services"
        )

        estimated_salary = st.number_input(
            "Estimated Salary ($)",
            min_value=0.0,
            max_value=200000.0,
            value=100000.0,
            step=1000.0
        )

    st.markdown("---")

    # Model selection
    st.markdown("### Select Prediction Model")

    model_options = list(models.keys())
    selected_model = st.selectbox(
        "Choose Model",
        options=model_options,
        index=model_options.index('Random Forest Optimized') if 'Random Forest Optimized' in model_options else 0,
        help="Different models may give slightly different predictions"
    )

    # Predict button
    if st.button("Predict Churn Risk", type="primary", use_container_width=True):
        with st.spinner("Analyzing customer data..."):
            time.sleep(0.5)  # Simulate processing

            # Prepare input data
            input_data = {
                'CreditScore': credit_score,
                'Geography': geography,
                'Gender': gender,
                'Age': age,
                'Tenure': tenure,
                'Balance': balance,
                'NumOfProducts': num_products,
                'HasCrCard': 1 if has_credit_card else 0,
                'IsActiveMember': 1 if is_active_member else 0,
                'EstimatedSalary': estimated_salary
            }

            # Validate input
            is_valid, error_msg = validate_input_data(input_data)
            if not is_valid:
                st.error(f"Invalid input: {error_msg}")
                return

            # Preprocess
            X = preprocess_input(input_data, scaler, label_encoder_gender, feature_names)

            # Predict
            model = models[selected_model]
            prediction, probability = predict_churn(model, X, selected_model)

            # Display results
            st.markdown("---")
            st.markdown("## Prediction Results")

            # Risk score with progress bar
            col1, col2, col3 = st.columns([2, 1, 1])

            with col1:
                st.markdown("### Churn Probability")
                st.progress(probability)
                st.markdown(f"<h1 style='text-align: center; color: #dc2626;'>{probability*100:.1f}%</h1>",
                           unsafe_allow_html=True)

            with col2:
                risk_category, risk_class = get_risk_category(probability)
                st.markdown("### Risk Level")
                st.markdown(f"<div class='{risk_class}'>{risk_category}</div>",
                           unsafe_allow_html=True)

            with col3:
                st.markdown("### Prediction")
                if prediction == 1:
                    st.markdown("<div class='risk-high'>WILL LEAVE</div>", unsafe_allow_html=True)
                else:
                    st.markdown("<div class='risk-low'>WILL STAY</div>", unsafe_allow_html=True)

            st.markdown("---")

            # Customer value analysis
            st.markdown("### Customer Value Analysis")

            clv = calculate_clv(balance, num_products, tenure)
            revenue_at_risk = clv * probability
            retention_cost = 50
            retention_roi = revenue_at_risk - retention_cost

            col1, col2, col3, col4 = st.columns(4)

            with col1:
                st.metric(
                    "Estimated Lifetime Value",
                    format_currency(clv),
                    help="Expected revenue from this customer"
                )

            with col2:
                st.metric(
                    "Revenue at Risk",
                    format_currency(revenue_at_risk),
                    delta=f"-{probability*100:.0f}%",
                    delta_color="inverse",
                    help="Potential loss if customer churns"
                )

            with col3:
                st.metric(
                    "Retention Campaign Cost",
                    format_currency(retention_cost),
                    help="Estimated cost to retain customer"
                )

            with col4:
                st.metric(
                    "Retention ROI",
                    format_currency(retention_roi),
                    delta=f"+{(retention_roi/retention_cost)*100:.0f}%",
                    help="Net benefit of retention campaign"
                )

            if probability > 0.6:
                st.error(f"""
                **HIGH RISK ALERT**: This customer has a {probability*100:.0f}% chance of churning!
                Immediate intervention is required. Revenue at risk: {format_currency(revenue_at_risk)}
                """)
            elif probability > 0.3:
                st.warning(f"""
                **MEDIUM RISK**: This customer shows warning signs of potential churn.
                Proactive engagement recommended. Revenue at risk: {format_currency(revenue_at_risk)}
                """)
            else:
                st.success(f"""
                **LOW RISK**: This customer is likely to remain with the bank.
                Continue standard relationship management.
                """)

            st.markdown("---")

            # Personalized recommendations
            st.markdown("### Personalized Retention Strategies")

            recommendations = get_retention_recommendations(input_data, probability)

            for i, rec in enumerate(recommendations, 1):
                st.markdown(f"""
                <div class="recommendation-box">
                    <strong>Strategy {i}:</strong><br>
                    {rec}
                </div>
                """, unsafe_allow_html=True)

            st.markdown("---")

            # Feature importance (for tree-based models)
            if selected_model in ['Random Forest', 'Random Forest Optimized', 'XGBoost',
                                  'XGBoost Optimized', 'Gradient Boosting', 'Decision Tree']:
                st.markdown("### Key Factors Influencing This Prediction")

                feature_importance = model.feature_importances_
                importance_df = pd.DataFrame({
                    'Feature': feature_names,
                    'Importance': feature_importance
                }).sort_values('Importance', ascending=False).head(10)

                fig = px.bar(
                    importance_df,
                    y='Feature',
                    x='Importance',
                    orientation='h',
                    title='Top 10 Most Important Features',
                    labels={'Importance': 'Importance Score'},
                    color='Importance',
                    color_continuous_scale='Blues'
                )
                fig.update_layout(height=400)
                st.plotly_chart(fig, use_container_width=True)

            # Customer profile comparison
            st.markdown("### Customer Profile Analysis")

            try:
                df = load_data()
                loyal_customers = df[df['Exited'] == 0]

                comparison_features = ['Age', 'CreditScore', 'Balance', 'Tenure', 'NumOfProducts']

                customer_values = [
                    age,
                    credit_score,
                    balance / 1000,  # Scale for better visualization
                    tenure,
                    num_products
                ]

                avg_loyal = [
                    loyal_customers['Age'].mean(),
                    loyal_customers['CreditScore'].mean(),
                    loyal_customers['Balance'].mean() / 1000,
                    loyal_customers['Tenure'].mean(),
                    loyal_customers['NumOfProducts'].mean()
                ]

                fig = go.Figure()

                fig.add_trace(go.Scatterpolar(
                    r=customer_values,
                    theta=comparison_features,
                    fill='toself',
                    name='This Customer'
                ))

                fig.add_trace(go.Scatterpolar(
                    r=avg_loyal,
                    theta=comparison_features,
                    fill='toself',
                    name='Average Loyal Customer'
                ))

                fig.update_layout(
                    polar=dict(radialaxis=dict(visible=True)),
                    showlegend=True,
                    title="Customer Profile vs Average Loyal Customer",
                    height=500
                )

                st.plotly_chart(fig, use_container_width=True)

                st.info("""
                **How to interpret**: The closer the customer profile matches the average loyal customer,
                the lower the churn risk. Significant deviations indicate areas for targeted intervention.
                """)

            except Exception as e:
                st.warning("Could not load comparison data.")


def page_analytics():
    """Page 3: Data Analytics - EDA, clustering, and patterns"""
    st.title("Data Analytics & Insights")
    st.markdown("### Comprehensive analysis of customer data, segmentation, and churn patterns")

    st.markdown("---")

    # Load data
    try:
        df = load_data()
    except Exception as e:
        st.error(f"Error loading data: {str(e)}")
        st.warning("Please ensure Churn_Modelling.csv is in the data/ directory.")
        return

    # Create tabs
    tab1, tab2, tab3, tab4 = st.tabs([
        "Dataset Overview",
        "Exploratory Analysis",
        "Customer Segmentation",
        "Churn Patterns"
    ])

    with tab1:
        st.markdown("## Dataset Overview")

        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.metric("Total Customers", f"{len(df):,}")
        with col2:
            st.metric("Features", df.shape[1])
        with col3:
            churn_count = df['Exited'].sum()
            st.metric("Churned Customers", f"{churn_count:,}")
        with col4:
            churn_rate = df['Exited'].mean() * 100
            st.metric("Churn Rate", f"{churn_rate:.2f}%")

        st.markdown("---")

        # Display first rows
        st.markdown("### Sample Data")
        st.dataframe(df.head(10), use_container_width=True, height=400)

        st.markdown("---")

        # Statistical summary
        st.markdown("### Statistical Summary")
        st.dataframe(df.describe(), use_container_width=True)

        st.markdown("---")

        # Missing values
        st.markdown("### Data Quality Check")
        missing = df.isnull().sum()
        if missing.sum() == 0:
            st.success("No missing values detected! Dataset is clean.")
        else:
            st.warning("Missing values found:")
            st.dataframe(missing[missing > 0])

        # Target distribution
        st.markdown("### Target Variable Distribution")

        target_counts = df['Exited'].value_counts()

        fig = px.pie(
            values=target_counts.values,
            names=['Retained (0)', 'Churned (1)'],
            title='Customer Churn Distribution',
            color_discrete_sequence=['#10b981', '#ef4444']
        )
        st.plotly_chart(fig, use_container_width=True)

    with tab2:
        st.markdown("## Exploratory Data Analysis")

        # Churn by Geography
        st.markdown("### Churn Rate by Geography")
        geo_churn = df.groupby('Geography')['Exited'].agg(['sum', 'count'])
        geo_churn['churn_rate'] = (geo_churn['sum'] / geo_churn['count'] * 100)

        fig = px.bar(
            geo_churn.reset_index(),
            x='Geography',
            y='churn_rate',
            title='Churn Rate by Country',
            labels={'churn_rate': 'Churn Rate (%)'},
            color='churn_rate',
            color_continuous_scale='Reds',
            text='churn_rate'
        )
        fig.update_traces(texttemplate='%{text:.1f}%', textposition='outside')
        st.plotly_chart(fig, use_container_width=True)

        st.info("Germany shows significantly higher churn rate - requires targeted retention strategy!")

        # Churn by Gender
        st.markdown("### Churn Rate by Gender")
        gender_churn = df.groupby('Gender')['Exited'].agg(['sum', 'count'])
        gender_churn['churn_rate'] = (gender_churn['sum'] / gender_churn['count'] * 100)

        fig = px.bar(
            gender_churn.reset_index(),
            x='Gender',
            y='churn_rate',
            title='Churn Rate by Gender',
            labels={'churn_rate': 'Churn Rate (%)'},
            color='churn_rate',
            color_continuous_scale='Oranges',
            text='churn_rate'
        )
        fig.update_traces(texttemplate='%{text:.1f}%', textposition='outside')
        st.plotly_chart(fig, use_container_width=True)

        # Age distribution
        st.markdown("### Age Distribution by Churn Status")

        fig = px.histogram(
            df,
            x='Age',
            color='Exited',
            title='Age Distribution (Retained vs Churned)',
            labels={'Exited': 'Status'},
            nbins=30,
            barmode='overlay',
            opacity=0.7,
            color_discrete_map={0: '#3b82f6', 1: '#ef4444'}
        )
        st.plotly_chart(fig, use_container_width=True)

        st.info("Older customers (45+) show higher churn rates - age-targeted retention needed!")

        # Balance distribution
        st.markdown("### Balance Distribution by Churn Status")

        fig = px.box(
            df,
            x='Exited',
            y='Balance',
            title='Account Balance Distribution',
            labels={'Exited': 'Churn Status', 'Balance': 'Account Balance ($)'},
            color='Exited',
            color_discrete_map={0: '#10b981', 1: '#ef4444'}
        )
        st.plotly_chart(fig, use_container_width=True)

        # Credit Score distribution
        st.markdown("### Credit Score Distribution by Churn Status")

        fig = px.violin(
            df,
            x='Exited',
            y='CreditScore',
            title='Credit Score Distribution',
            labels={'Exited': 'Churn Status', 'CreditScore': 'Credit Score'},
            color='Exited',
            color_discrete_map={0: '#3b82f6', 1: '#f59e0b'},
            box=True
        )
        st.plotly_chart(fig, use_container_width=True)

        # Churn by Number of Products
        st.markdown("### Churn Rate by Number of Products")

        products_churn = df.groupby('NumOfProducts')['Exited'].agg(['sum', 'count'])
        products_churn['churn_rate'] = (products_churn['sum'] / products_churn['count'] * 100)

        fig = px.bar(
            products_churn.reset_index(),
            x='NumOfProducts',
            y='churn_rate',
            title='Churn Rate by Number of Products',
            labels={'churn_rate': 'Churn Rate (%)', 'NumOfProducts': 'Number of Products'},
            color='churn_rate',
            color_continuous_scale='RdYlGn_r',
            text='churn_rate'
        )
        fig.update_traces(texttemplate='%{text:.1f}%', textposition='outside')
        st.plotly_chart(fig, use_container_width=True)

        st.warning("Customers with 3-4 products show unexpectedly high churn - investigate product bundling issues!")

        # Churn by Active Member
        st.markdown("### Churn Rate by Activity Status")

        active_churn = df.groupby('IsActiveMember')['Exited'].agg(['sum', 'count'])
        active_churn['churn_rate'] = (active_churn['sum'] / active_churn['count'] * 100)

        fig = px.bar(
            active_churn.reset_index(),
            x='IsActiveMember',
            y='churn_rate',
            title='Churn Rate: Active vs Inactive Members',
            labels={'churn_rate': 'Churn Rate (%)', 'IsActiveMember': 'Active Status'},
            color='churn_rate',
            color_continuous_scale='Reds',
            text='churn_rate'
        )
        fig.update_traces(texttemplate='%{text:.1f}%', textposition='outside')
        fig.update_xaxes(ticktext=['Inactive', 'Active'], tickvals=[0, 1])
        st.plotly_chart(fig, use_container_width=True)

        st.error("Inactive members are 2x more likely to churn - re-engagement is critical!")

        # Correlation heatmap
        st.markdown("### Feature Correlation Heatmap")

        numeric_cols = df.select_dtypes(include=[np.number]).columns
        corr_matrix = df[numeric_cols].corr()

        fig = px.imshow(
            corr_matrix,
            title='Feature Correlation Matrix',
            labels=dict(color="Correlation"),
            color_continuous_scale='RdBu_r',
            aspect="auto",
            zmin=-1,
            zmax=1
        )
        fig.update_layout(height=700)
        st.plotly_chart(fig, use_container_width=True)

    with tab3:
        st.markdown("## Customer Segmentation (K-Means Clustering)")
        st.markdown("### CO4: Unsupervised Learning - Identifying distinct customer groups")

        # Load cluster profiles
        cluster_profiles = load_cluster_profiles()

        if cluster_profiles is not None:
            # Cluster overview
            st.markdown("### Cluster Overview")

            col1, col2, col3 = st.columns(3)

            with col1:
                st.metric("Total Clusters", len(cluster_profiles))
            with col2:
                avg_size = cluster_profiles['Size'].mean()
                st.metric("Avg Cluster Size", f"{avg_size:.0f}")
            with col3:
                high_risk_clusters = (cluster_profiles['Churn_Rate'] > 30).sum()
                st.metric("High-Risk Clusters", high_risk_clusters)

            st.markdown("---")

            # Cluster profiles table
            st.markdown("### Cluster Profiles")

            # Add cluster names
            cluster_names = []
            for idx, row in cluster_profiles.iterrows():
                name, _, _ = interpret_cluster(row, idx)
                cluster_names.append(name)

            cluster_profiles['Profile_Name'] = cluster_names

            # Reorder columns
            display_cols = ['Cluster', 'Profile_Name', 'Size', 'Avg_Age', 'Avg_CreditScore',
                          'Avg_Balance', 'Avg_Tenure', 'Avg_NumOfProducts', 'Churn_Rate']

            st.dataframe(
                cluster_profiles[display_cols].style.background_gradient(
                    subset=['Churn_Rate'],
                    cmap='RdYlGn_r'
                ).format({
                    'Avg_Age': '{:.1f}',
                    'Avg_CreditScore': '{:.0f}',
                    'Avg_Balance': '${:,.2f}',
                    'Avg_Tenure': '{:.1f}',
                    'Avg_NumOfProducts': '{:.2f}',
                    'Churn_Rate': '{:.2f}%'
                }),
                use_container_width=True,
                height=300
            )

            st.markdown("---")

            # Cluster distribution
            st.markdown("### Cluster Size Distribution")

            fig = px.pie(
                cluster_profiles,
                values='Size',
                names='Profile_Name',
                title='Customer Distribution Across Clusters',
                hole=0.4
            )
            st.plotly_chart(fig, use_container_width=True)

            # 3D visualization
            st.markdown("### 3D Cluster Visualization")

            st.info("""
            Interactive 3D scatter plot showing customer clusters based on Age, Balance, and Tenure.
            Use mouse to rotate and zoom. Each point represents a customer, colored by cluster.
            """)

            try:
                df_with_clusters = load_data()

                # Check if cluster column exists
                if 'Cluster' in df_with_clusters.columns:
                    fig = px.scatter_3d(
                        df_with_clusters,
                        x='Age',
                        y='Balance',
                        z='Tenure',
                        color='Cluster',
                        title='Customer Segmentation - 3D View',
                        labels={'Cluster': 'Cluster ID'},
                        opacity=0.7,
                        hover_data=['CreditScore', 'NumOfProducts', 'Exited']
                    )
                    fig.update_layout(height=700)
                    st.plotly_chart(fig, use_container_width=True)
                else:
                    st.warning("Cluster assignments not found. Please run train_models.py first.")
            except:
                st.warning("3D visualization not available. Please run train_models.py first.")

            st.markdown("---")

            # Detailed cluster descriptions
            st.markdown("### Cluster Descriptions & Retention Strategies")

            for idx, row in cluster_profiles.iterrows():
                name, description, strategies = interpret_cluster(row, idx)

                with st.expander(f"Cluster {idx}: {name}"):
                    st.markdown(f"**Description:** {description}")

                    st.markdown("**Characteristics:**")
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("Customers", f"{int(row['Size']):,}")
                        st.metric("Avg Age", f"{row['Avg_Age']:.1f}")
                    with col2:
                        st.metric("Avg Balance", f"${row['Avg_Balance']:,.0f}")
                        st.metric("Avg Tenure", f"{row['Avg_Tenure']:.1f} yrs")
                    with col3:
                        st.metric("Avg Products", f"{row['Avg_NumOfProducts']:.2f}")
                        st.metric("Churn Rate", f"{row['Churn_Rate']:.1f}%")

                    st.markdown("**Recommended Retention Strategies:**")
                    for i, strategy in enumerate(strategies, 1):
                        st.markdown(f"{i}. {strategy}")

        else:
            st.warning("Cluster profiles not found. Please run train_models.py first to perform clustering.")

    with tab4:
        st.markdown("## Churn Patterns (Association Rule Mining)")
        st.markdown("### CO4: Unsupervised Learning - Discovering hidden patterns with Apriori algorithm")

        # Load association rules
        rules = load_association_rules()

        if rules is not None and len(rules) > 0:
            st.success(f"Found {len(rules)} association rules predicting customer churn")

            st.markdown("---")

            # Filters
            st.markdown("### Filter Rules")

            col1, col2, col3 = st.columns(3)

            with col1:
                min_confidence = st.slider(
                    "Minimum Confidence",
                    min_value=0.6,
                    max_value=1.0,
                    value=0.7,
                    step=0.05,
                    help="Higher confidence = more reliable rules"
                )

            with col2:
                min_support = st.slider(
                    "Minimum Support",
                    min_value=0.01,
                    max_value=0.10,
                    value=0.02,
                    step=0.01,
                    help="Higher support = more frequent patterns"
                )

            with col3:
                top_n = st.slider(
                    "Number of Rules to Display",
                    min_value=5,
                    max_value=30,
                    value=15
                )

            # Filter rules
            filtered_rules = rules[
                (rules['confidence'] >= min_confidence) &
                (rules['support'] >= min_support)
            ].sort_values('confidence', ascending=False).head(top_n)

            st.markdown(f"### Top {len(filtered_rules)} Association Rules")

            # Display rules in table
            display_rules = filtered_rules[['antecedents', 'consequents', 'support',
                                          'confidence', 'lift']].copy()

            # Format for display
            display_rules['support'] = display_rules['support'].apply(lambda x: f"{x:.3f}")
            display_rules['confidence'] = display_rules['confidence'].apply(lambda x: f"{x:.1%}")
            display_rules['lift'] = display_rules['lift'].apply(lambda x: f"{x:.2f}")

            st.dataframe(
                display_rules.style.apply(
                    lambda x: ['background-color: #fee2e2' if float(conf.strip('%'))/100 > 0.8
                              else '' for conf in x],
                    subset=['confidence']
                ),
                use_container_width=True,
                height=400
            )

            st.markdown("---")

            # Top rules by confidence chart
            st.markdown("### Top 10 Rules by Confidence")

            top_10 = filtered_rules.head(10).copy()
            top_10['rule_label'] = top_10.index.astype(str)

            fig = px.bar(
                top_10,
                x='confidence',
                y='rule_label',
                orientation='h',
                title='Confidence Scores of Top Rules',
                labels={'confidence': 'Confidence', 'rule_label': 'Rule #'},
                color='confidence',
                color_continuous_scale='Reds',
                text='confidence'
            )
            fig.update_traces(texttemplate='%{text:.1%}', textposition='outside')
            fig.update_layout(height=500)
            st.plotly_chart(fig, use_container_width=True)

            st.markdown("---")

            # Business insights
            st.markdown("### Key Business Insights from Association Rules")

            insights = [
                {
                    'title': 'Geographic Risk Pattern',
                    'description': 'German customers with single product and inactive status show 75%+ churn rate',
                    'action': 'Launch targeted re-engagement campaign in Germany focusing on product bundling',
                    'priority': 'HIGH'
                },
                {
                    'title': 'Inactive Member Risk',
                    'description': 'Customers who are not active members are 2-3x more likely to churn across all segments',
                    'action': 'Implement automated digital engagement triggers for inactive accounts',
                    'priority': 'HIGH'
                },
                {
                    'title': 'Single Product Vulnerability',
                    'description': 'Customers using only 1 product have significantly higher churn, especially when combined with low balance',
                    'action': 'Create attractive product bundle offers with first 3 months at reduced fees',
                    'priority': 'MEDIUM'
                },
                {
                    'title': 'Senior Customer Attrition',
                    'description': 'Customers over 50 with short tenure (<3 years) show elevated churn risk',
                    'action': 'Assign dedicated relationship managers to senior new customers',
                    'priority': 'MEDIUM'
                },
                {
                    'title': 'Low Balance Alert',
                    'description': 'Accounts with balance below $50k combined with no credit card predict 60%+ churn',
                    'action': 'Offer fee waivers and credit card promotions to low-balance accounts',
                    'priority': 'MEDIUM'
                }
            ]

            for insight in insights:
                priority_color = {
                    'HIGH': '#dc2626',
                    'MEDIUM': '#f59e0b',
                    'LOW': '#10b981'
                }

                st.markdown(f"""
                <div class="prediction-box">
                    <h4>{insight['title']} <span style="background-color: {priority_color[insight['priority']]}; color: white; padding: 3px 10px; border-radius: 5px; font-size: 12px;">{insight['priority']}</span></h4>
                    <p><strong>Pattern:</strong> {insight['description']}</p>
                    <p><strong>Recommended Action:</strong> {insight['action']}</p>
                </div>
                """, unsafe_allow_html=True)

            st.markdown("---")

            # Interpretation guide
            with st.expander("How to Interpret Association Rules"):
                st.markdown("""
                ### Understanding Association Rules

                Association rules follow the format: **IF [Conditions] THEN [Outcome]**

                **Key Metrics:**

                1. **Support**: How often does this pattern occur in the data?
                   - Higher support = more frequent pattern
                   - Example: Support of 0.05 means 5% of customers match this pattern

                2. **Confidence**: When conditions are met, how likely is the outcome?
                   - Higher confidence = more reliable rule
                   - Example: Confidence of 0.75 means 75% of customers matching conditions will churn

                3. **Lift**: How much more likely is the outcome compared to random?
                   - Lift > 1: Pattern increases likelihood
                   - Lift = 1: No association
                   - Lift < 1: Pattern decreases likelihood
                   - Example: Lift of 2.5 means outcome is 2.5x more likely than average

                **Business Application:**

                Use these rules to:
                - Identify high-risk customer profiles before they churn
                - Design targeted intervention campaigns
                - Allocate retention budget to highest-impact segments
                - Optimize product offerings based on churn triggers
                """)

        else:
            st.warning("Association rules not found. Please run train_models.py first to perform association rule mining.")


def page_model_performance():
    """Page 4: Model Performance - Comprehensive model comparison"""
    st.title("Model Performance & Evaluation")
    st.markdown("### CO3: Supervised Learning - Comparing 7 classification models")

    st.markdown("---")

    # Load results
    results = load_results()

    if results is None:
        st.error("Model results not found. Please run train_models.py first.")
        return

    # Load models for additional analysis
    try:
        models, scaler, label_encoder_gender, feature_names = load_models()
        df = load_data()
    except:
        st.warning("Could not load all models/data for detailed analysis.")
        models = None
        df = None

    # Section 1: Model Comparison Table
    st.markdown("## Model Performance Comparison")

    comparison_data = []
    for model_name, metrics in results['models'].items():
        comparison_data.append({
            'Model': model_name,
            'Accuracy': metrics['accuracy'],
            'Precision': metrics['precision'],
            'Recall': metrics['recall'],
            'F1-Score': metrics['f1_score'],
            'ROC-AUC': metrics['roc_auc'],
            'Training Time (s)': metrics['training_time']
        })

    comparison_df = pd.DataFrame(comparison_data)

    # Highlight best values
    st.dataframe(
        comparison_df.style.highlight_max(
            subset=['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC'],
            color='lightgreen'
        ).highlight_min(
            subset=['Training Time (s)'],
            color='lightblue'
        ).format({
            'Accuracy': '{:.4f}',
            'Precision': '{:.4f}',
            'Recall': '{:.4f}',
            'F1-Score': '{:.4f}',
            'ROC-AUC': '{:.4f}',
            'Training Time (s)': '{:.2f}'
        }),
        use_container_width=True,
        height=400
    )

    st.info("""
    **Why Recall is Most Important in Churn Prediction:**

    - **Recall** measures our ability to catch customers who will actually churn
    - Missing a churner (False Negative) costs $1,500 in lost lifetime value
    - False alarm (False Positive) only costs $50 in wasted retention offer
    - Therefore, we prioritize Recall over Precision - better to be safe than sorry!
    """)

    st.markdown("---")

    # Section 2: Metrics Visualization
    st.markdown("## Metrics Comparison")

    metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']

    fig = go.Figure()

    for metric in metrics_to_plot:
        fig.add_trace(go.Bar(
            name=metric,
            x=comparison_df['Model'],
            y=comparison_df[metric],
            text=comparison_df[metric].apply(lambda x: f'{x:.3f}'),
            textposition='outside'
        ))

    fig.update_layout(
        title='Model Performance Across All Metrics',
        xaxis_title='Model',
        yaxis_title='Score',
        barmode='group',
        height=600,
        yaxis=dict(range=[0, 1.1])
    )

    st.plotly_chart(fig, use_container_width=True)

    # Training time comparison
    st.markdown("### Training Time Comparison")

    fig = px.bar(
        comparison_df.sort_values('Training Time (s)'),
        x='Training Time (s)',
        y='Model',
        orientation='h',
        title='Model Training Time (Lower is Better)',
        labels={'Training Time (s)': 'Training Time (seconds)'},
        color='Training Time (s)',
        color_continuous_scale='Viridis',
        text='Training Time (s)'
    )
    fig.update_traces(texttemplate='%{text:.2f}s', textposition='outside')
    st.plotly_chart(fig, use_container_width=True)

    st.markdown("---")

    # Section 3: Best Model Recommendation
    st.markdown("## Model Selection Recommendations")

    best_recall = comparison_df.loc[comparison_df['Recall'].idxmax()]
    best_accuracy = comparison_df.loc[comparison_df['Accuracy'].idxmax()]
    best_precision = comparison_df.loc[comparison_df['Precision'].idxmax()]
    best_f1 = comparison_df.loc[comparison_df['F1-Score'].idxmax()]
    best_auc = comparison_df.loc[comparison_df['ROC-AUC'].idxmax()]
    fastest = comparison_df.loc[comparison_df['Training Time (s)'].idxmin()]

    col1, col2, col3 = st.columns(3)

    with col1:
        st.markdown("""
        <div class="prediction-box">
            <h4>Best for Catching Churners (Recall)</h4>
            <h3 style="color: #dc2626;">{}</h3>
            <p>Recall: {:.4f}</p>
            <p>Use when: Missing a churner is very costly</p>
        </div>
        """.format(best_recall['Model'], best_recall['Recall']), unsafe_allow_html=True)

    with col2:
        st.markdown("""
        <div class="prediction-box">
            <h4>Best Overall (ROC-AUC)</h4>
            <h3 style="color: #16a34a;">{}</h3>
            <p>ROC-AUC: {:.4f}</p>
            <p>Use when: Balanced performance needed</p>
        </div>
        """.format(best_auc['Model'], best_auc['ROC-AUC']), unsafe_allow_html=True)

    with col3:
        st.markdown("""
        <div class="prediction-box">
            <h4>Best Precision</h4>
            <h3 style="color: #3b82f6;">{}</h3>
            <p>Precision: {:.4f}</p>
            <p>Use when: Avoiding false alarms is priority</p>
        </div>
        """.format(best_precision['Model'], best_precision['Precision']), unsafe_allow_html=True)

    st.markdown("---")

    # Section 4: Business Cost Analysis
    st.markdown("## Business Cost Analysis")

    st.info("""
    **Cost Structure:**
    - False Negative (Missed Churner): $1,500 lost revenue
    - False Positive (False Alarm): $50 wasted retention offer
    - True Positive (Correctly Predicted Churner): Potential $1,450 savings
    """)

    st.warning("Note: Detailed confusion matrices and business cost analysis require running train_models.py")

    st.markdown("---")

    # Section 5: Model-specific insights
    st.markdown("## Model-Specific Analysis")

    with st.expander("Logistic Regression"):
        st.markdown("""
        **Strengths:**
        - Fast training and prediction
        - Interpretable coefficients
        - Works well as baseline model
        - Good for understanding feature importance

        **Weaknesses:**
        - Linear decision boundary limitation
        - May underperform on complex patterns
        - Assumes feature independence

        **Best Use Case:** Quick baseline model, interpretability required
        """)

    with st.expander("Decision Tree"):
        st.markdown("""
        **Strengths:**
        - Highly interpretable (can visualize tree)
        - Captures non-linear relationships
        - No feature scaling required
        - Handles mixed data types well

        **Weaknesses:**
        - Prone to overfitting
        - Unstable (small data changes = different tree)
        - Lower performance than ensemble methods

        **Best Use Case:** Explainability to non-technical stakeholders
        """)

    with st.expander("Random Forest & Random Forest Optimized"):
        st.markdown("""
        **Strengths:**
        - Excellent performance on tabular data
        - Reduces overfitting through ensemble
        - Provides feature importance
        - Handles imbalanced data well

        **Weaknesses:**
        - Slower than single models
        - Less interpretable than single tree
        - Requires more memory

        **Best Use Case:** Production deployment, high accuracy needed

        **Grid Search Impact:** Optimization improved recall by tuning n_estimators, max_depth, and min_samples_split
        """)

    with st.expander("XGBoost & XGBoost Optimized"):
        st.markdown("""
        **Strengths:**
        - Often best performer on structured data
        - Built-in regularization prevents overfitting
        - Handles missing values
        - Fast training with parallel processing

        **Weaknesses:**
        - Many hyperparameters to tune
        - Can overfit if not regularized
        - Requires careful parameter tuning

        **Best Use Case:** Maximum performance, competitions, production systems

        **Grid Search Impact:** Optimization tuned learning_rate, max_depth, n_estimators, subsample
        """)

    with st.expander("SVM (Support Vector Machine)"):
        st.markdown("""
        **Strengths:**
        - Effective in high-dimensional spaces
        - Memory efficient
        - Kernel trick for non-linear patterns

        **Weaknesses:**
        - Slow on large datasets
        - Sensitive to feature scaling
        - Difficult to interpret
        - Longer training time

        **Best Use Case:** Small to medium datasets, when data is linearly separable
        """)

    with st.expander("Gradient Boosting"):
        st.markdown("""
        **Strengths:**
        - Sequential learning improves weak models
        - Often high accuracy
        - Good feature importance estimates

        **Weaknesses:**
        - Slower training (sequential, not parallel)
        - Can overfit with too many iterations
        - Sensitive to hyperparameters

        **Best Use Case:** When accuracy is paramount and training time is acceptable
        """)

    with st.expander("Neural Network"):
        st.markdown("""
        **Architecture:**
        - Input Layer: 20+ features
        - Hidden Layer 1: 128 neurons (ReLU) + BatchNorm + Dropout(0.3)
        - Hidden Layer 2: 64 neurons (ReLU) + BatchNorm + Dropout(0.3)
        - Hidden Layer 3: 32 neurons (ReLU) + BatchNorm + Dropout(0.2)
        - Output Layer: 1 neuron (Sigmoid)

        **Strengths:**
        - Captures complex non-linear interactions
        - Can learn hierarchical features
        - Scales to large datasets
        - Flexible architecture

        **Weaknesses:**
        - Requires more data
        - Longer training time
        - Black box (hard to interpret)
        - Requires careful tuning

        **Best Use Case:** Large datasets, complex patterns, deep feature interactions

        **Training Strategy:** Early stopping prevents overfitting, dropout regularizes, BatchNorm stabilizes training
        """)

    st.markdown("---")

    # Section 6: Final Recommendation
    st.markdown("## Final Recommendation")

    st.success(f"""
    ### Recommended Model for Production: **{best_recall['Model']}**

    **Justification:**
    - Highest Recall ({best_recall['Recall']:.4f}): Best at catching customers who will churn
    - Strong ROC-AUC ({best_recall['ROC-AUC']:.4f}): Good overall discrimination
    - Acceptable training time ({best_recall['Training Time (s)']:.2f}s): Can retrain regularly

    **Business Impact:**
    - Maximizes revenue retention by identifying at-risk customers
    - Minimizes costly false negatives (missed churners)
    - Enables proactive intervention before customers leave

    **Implementation Strategy:**
    1. Deploy this model for daily batch predictions
    2. Set alert threshold at 60% churn probability for high-risk customers
    3. Trigger automated retention campaigns for medium-risk (30-60%)
    4. Monitor model performance monthly and retrain quarterly
    5. A/B test retention strategies on different risk segments
    """)


def page_batch_predictions():
    """Page 5: Batch Predictions & Retention Strategy"""
    st.title("Batch Predictions & Retention Strategy")
    st.markdown("### Analyze entire customer database and prioritize retention efforts")

    st.markdown("---")

    # Load models
    try:
        models, scaler, label_encoder_gender, feature_names = load_models()
    except Exception as e:
        st.error(f"Error loading models: {str(e)}")
        st.warning("Please run train_models.py first.")
        return

    # Section 1: Upload & Predict
    st.markdown("## Upload Customer Database")

    st.info("""
    **Upload Instructions:**
    - CSV file format required
    - Must contain these columns: CreditScore, Geography, Gender, Age, Tenure, Balance,
      NumOfProducts, HasCrCard, IsActiveMember, EstimatedSalary
    - Optional columns: CustomerId, Surname (for identification)
    """)

    uploaded_file = st.file_uploader(
        "Choose CSV file",
        type=['csv'],
        help="Upload your customer database"
    )

    # Model selection for batch processing
    model_options = list(models.keys())
    selected_model = st.selectbox(
        "Select Model for Predictions",
        options=model_options,
        index=model_options.index('Random Forest Optimized') if 'Random Forest Optimized' in model_options else 0
    )

    if uploaded_file is not None:
        # Read uploaded file
        # try:
            df_upload = pd.read_csv(uploaded_file)

            st.success(f"File uploaded successfully! Found {len(df_upload)} customers.")

            # Preview
            st.markdown("### Data Preview")
            st.dataframe(df_upload.head(10), use_container_width=True, height=300)

            # Process predictions button
            if st.button("Analyze All Customers", type="primary", use_container_width=True):
                with st.spinner("Processing predictions for all customers..."):
                    # Store results
                    predictions = []
                    probabilities = []

                    # Process each customer
                    for idx, row in df_upload.iterrows():
                        try:
                            input_data = {
                                'CreditScore': row['CreditScore'],
                                'Geography': row['Geography'],
                                'Gender': row['Gender'],
                                'Age': row['Age'],
                                'Tenure': row['Tenure'],
                                'Balance': row['Balance'],
                                'NumOfProducts': row['NumOfProducts'],
                                'HasCrCard': row['HasCrCard'],
                                'IsActiveMember': row['IsActiveMember'],
                                'EstimatedSalary': row['EstimatedSalary']
                            }

                            # Preprocess and predict
                            X = preprocess_input(input_data, scaler, label_encoder_gender, feature_names)
                            model = models[selected_model]
                            pred, prob = predict_churn(model, X, selected_model)

                            predictions.append(pred)
                            probabilities.append(prob)

                        except Exception as e:
                            predictions.append(None)
                            probabilities.append(None)

                    # Add results to dataframe
                    df_upload['Churn_Prediction'] = predictions
                    df_upload['Churn_Probability'] = probabilities

                    # Calculate CLV and risk
                    df_upload['CLV'] = df_upload.apply(
                        lambda row: calculate_clv(row['Balance'], row['NumOfProducts'], row['Tenure']),
                        axis=1
                    )
                    df_upload['Revenue_At_Risk'] = df_upload['CLV'] * df_upload['Churn_Probability']

                    # Risk categories
                    df_upload['Risk_Category'] = df_upload['Churn_Probability'].apply(
                        lambda x: 'HIGH' if x > 0.6 else ('MEDIUM' if x > 0.3 else 'LOW')
                    )

                    # Retention recommendations
                    df_upload['Recommendation'] = df_upload.apply(
                        lambda row: get_retention_recommendations(
                            {
                                'NumOfProducts': row['NumOfProducts'],
                                'IsActiveMember': row['IsActiveMember'],
                                'Balance': row['Balance'],
                                'HasCrCard': row['HasCrCard'],
                                'Age': row['Age'],
                                'Geography': row['Geography'],
                                'Tenure': row['Tenure']
                            },
                            row['Churn_Probability']
                        )[0] if row['Churn_Probability'] is not None else 'N/A',
                        axis=1
                    )

                    st.success("Analysis complete!")

                    # Store in session state
                    st.session_state['batch_results'] = df_upload

    # Display results if available
    if 'batch_results' in st.session_state:
        df_results = st.session_state['batch_results']

        st.markdown("---")

        # Section 2: Risk Stratification
        st.markdown("## Risk Stratification Summary")

        risk_counts = df_results['Risk_Category'].value_counts()
        total_revenue_risk = df_results['Revenue_At_Risk'].sum()

        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.metric(
                "Total Customers Analyzed",
                f"{len(df_results):,}"
            )

        with col2:
            high_risk_count = risk_counts.get('HIGH', 0)
            high_risk_pct = (high_risk_count / len(df_results)) * 100
            st.metric(
                "High Risk",
                f"{high_risk_count:,}",
                delta=f"{high_risk_pct:.1f}%"
            )

        with col3:
            medium_risk_count = risk_counts.get('MEDIUM', 0)
            medium_risk_pct = (medium_risk_count / len(df_results)) * 100
            st.metric(
                "Medium Risk",
                f"{medium_risk_count:,}",
                delta=f"{medium_risk_pct:.1f}%"
            )

        with col4:
            st.metric(
                "Total Revenue at Risk",
                format_currency(total_revenue_risk),
                delta=f"-{(total_revenue_risk / (df_results['CLV'].sum()))*100:.1f}%",
                delta_color="inverse"
            )

        # Risk distribution pie chart
        st.markdown("### Risk Distribution")

        fig = px.pie(
            values=risk_counts.values,
            names=risk_counts.index,
            title='Customer Risk Distribution',
            color=risk_counts.index,
            color_discrete_map={'LOW': '#10b981', 'MEDIUM': '#f59e0b', 'HIGH': '#ef4444'}
        )
        st.plotly_chart(fig, use_container_width=True)

        st.markdown("---")

        # Section 3: Retention Priority List
        st.markdown("## Retention Priority List")

        st.info("""
        **Priority Score Calculation:**
        Priority Score = Churn Probability √ó Customer Lifetime Value

        Focus retention efforts on customers with high priority scores - they have both
        high churn risk AND high value to the business.
        """)

        # Calculate priority score
        df_results['Priority_Score'] = df_results['Churn_Probability'] * df_results['CLV']

        # Sort by priority
        df_priority = df_results.sort_values('Priority_Score', ascending=False).head(50)

        # Display columns
        display_cols = ['CustomerId', 'Surname', 'Churn_Probability', 'Risk_Category',
                       'Balance', 'CLV', 'Priority_Score', 'Recommendation']

        # Check which columns exist
        available_cols = [col for col in display_cols if col in df_priority.columns]

        if 'CustomerId' not in df_priority.columns:
            df_priority['CustomerId'] = df_priority.index
        if 'Surname' not in df_priority.columns:
            df_priority['Surname'] = 'N/A'

        st.markdown("### Top 50 Priority Customers")

        st.dataframe(
            df_priority[['CustomerId', 'Surname', 'Churn_Probability', 'Risk_Category',
                        'Balance', 'Priority_Score', 'Recommendation']].style.background_gradient(
                subset=['Priority_Score'],
                cmap='Reds'
            ).format({
                'Churn_Probability': '{:.1%}',
                'Balance': '${:,.2f}',
                'Priority_Score': '{:.2f}'
            }),
            use_container_width=True,
            height=600
        )

        st.markdown("---")

        # Section 4: Campaign ROI Calculator
        st.markdown("## Retention Campaign ROI Calculator")

        st.markdown("### Campaign Parameters")

        col1, col2 = st.columns(2)

        with col1:
            campaign_budget = st.slider(
                "Total Campaign Budget ($)",
                min_value=10000,
                max_value=500000,
                value=50000,
                step=5000,
                help="Total budget available for retention campaigns"
            )

            cost_per_customer = st.slider(
                "Cost per Customer Contacted ($)",
                min_value=20,
                max_value=100,
                value=50,
                step=5,
                help="Average cost to execute retention offer per customer"
            )

        with col2:
            success_rate = st.slider(
                "Expected Retention Success Rate (%)",
                min_value=10,
                max_value=50,
                value=25,
                step=5,
                help="Percentage of contacted customers expected to be retained"
            ) / 100

            target_risk = st.radio(
                "Target Risk Level",
                options=['High Risk Only', 'Medium + High Risk'],
                help="Which customers to include in campaign"
            )

        # Calculate campaign metrics
        if target_risk == 'High Risk Only':
            target_customers = df_results[df_results['Risk_Category'] == 'HIGH']
        else:
            target_customers = df_results[df_results['Risk_Category'].isin(['HIGH', 'MEDIUM'])]

        customers_to_contact = min(int(campaign_budget / cost_per_customer), len(target_customers))
        actual_cost = customers_to_contact * cost_per_customer
        expected_retained = int(customers_to_contact * success_rate)

        # Calculate revenue saved
        top_targets = target_customers.nlargest(customers_to_contact, 'Priority_Score')
        expected_revenue_saved = top_targets['Revenue_At_Risk'].sum() * success_rate

        net_roi = expected_revenue_saved - actual_cost
        roi_percentage = (net_roi / actual_cost) * 100 if actual_cost > 0 else 0
        break_even_rate = (cost_per_customer / 1500) * 100  # Based on avg CLV

        st.markdown("---")
        st.markdown("### Campaign Results Projection")

        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.metric(
                "Customers to Contact",
                f"{customers_to_contact:,}",
                help="Number of customers that can be contacted with budget"
            )

        with col2:
            st.metric(
                "Expected Customers Retained",
                f"{expected_retained:,}",
                delta=f"{success_rate*100:.0f}% success rate"
            )

        with col3:
            st.metric(
                "Total Campaign Cost",
                format_currency(actual_cost)
            )

        with col4:
            st.metric(
                "Expected Revenue Saved",
                format_currency(expected_revenue_saved),
                delta=f"+{(expected_revenue_saved/actual_cost):.1f}x"
            )

        # ROI display
        st.markdown("### Return on Investment")

        col1, col2, col3 = st.columns(3)

        with col1:
            st.metric(
                "Net ROI",
                format_currency(net_roi),
                delta=f"{'Positive' if net_roi > 0 else 'Negative'} Return"
            )

        with col2:
            st.metric(
                "ROI Percentage",
                f"{roi_percentage:.0f}%",
                help="Return on investment percentage"
            )

        with col3:
            st.metric(
                "Break-even Success Rate",
                f"{break_even_rate:.1f}%",
                help="Minimum success rate needed to break even"
            )

        # ROI visualization
        roi_data = pd.DataFrame({
            'Category': ['Campaign Cost', 'Revenue Saved', 'Net Benefit'],
            'Amount': [actual_cost, expected_revenue_saved, net_roi],
            'Type': ['Cost', 'Revenue', 'Profit']
        })

        fig = px.bar(
            roi_data,
            x='Category',
            y='Amount',
            title='Campaign Financial Impact',
            labels={'Amount': 'Amount ($)'},
            color='Type',
            color_discrete_map={'Cost': '#ef4444', 'Revenue': '#10b981', 'Profit': '#3b82f6'},
            text='Amount'
        )
        fig.update_traces(texttemplate='$%{text:,.0f}', textposition='outside')
        fig.update_layout(height=500)
        st.plotly_chart(fig, use_container_width=True)

        if net_roi > 0:
            st.success(f"""
            **Campaign is Profitable!**

            Expected net benefit: {format_currency(net_roi)}

            For every $1 spent on retention, you'll save ${expected_revenue_saved/actual_cost:.2f} in revenue.
            Campaign should proceed as long as success rate exceeds {break_even_rate:.1f}%.
            """)
        else:
            st.warning(f"""
            **Campaign May Not Be Profitable**

            Expected net loss: {format_currency(abs(net_roi))}

            Consider:
            - Increasing success rate through better targeting
            - Reducing cost per customer
            - Focusing only on highest-priority customers
            - Improving retention offer quality
            """)

        st.markdown("---")

        # Section 5: Export Options
        st.markdown("## Export Results")

        col1, col2 = st.columns(2)

        with col1:
            # Full predictions CSV
            csv_full = df_results.to_csv(index=False)
            st.download_button(
                label="Download Full Predictions (CSV)",
                data=csv_full,
                file_name="customer_churn_predictions.csv",
                mime="text/csv",
                use_container_width=True
            )

        with col2:
            # Priority list CSV
            csv_priority = df_priority.to_csv(index=False)
            st.download_button(
                label="Download Retention Priority List (CSV)",
                data=csv_priority,
                file_name="retention_priority_list.csv",
                mime="text/csv",
                use_container_width=True
            )

        # Summary report
        summary_report = f"""
        BANK CUSTOMER CHURN PREDICTION - EXECUTIVE SUMMARY
        ================================================

        Analysis Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
        Model Used: {selected_model}

        CUSTOMER RISK PROFILE
        ---------------------
        Total Customers Analyzed: {len(df_results):,}
        High Risk Customers: {risk_counts.get('HIGH', 0):,} ({(risk_counts.get('HIGH', 0)/len(df_results)*100):.1f}%)
        Medium Risk Customers: {risk_counts.get('MEDIUM', 0):,} ({(risk_counts.get('MEDIUM', 0)/len(df_results)*100):.1f}%)
        Low Risk Customers: {risk_counts.get('LOW', 0):,} ({(risk_counts.get('LOW', 0)/len(df_results)*100):.1f}%)

        FINANCIAL IMPACT
        ----------------
        Total Customer Lifetime Value: {format_currency(df_results['CLV'].sum())}
        Total Revenue at Risk: {format_currency(total_revenue_risk)}
        Percentage at Risk: {(total_revenue_risk / df_results['CLV'].sum() * 100):.2f}%

        RECOMMENDED RETENTION CAMPAIGN
        ------------------------------
        Campaign Budget: {format_currency(campaign_budget)}
        Customers to Contact: {customers_to_contact:,}
        Cost per Customer: {format_currency(cost_per_customer)}
        Expected Success Rate: {success_rate*100:.0f}%

        Expected Customers Retained: {expected_retained:,}
        Expected Revenue Saved: {format_currency(expected_revenue_saved)}
        Total Campaign Cost: {format_currency(actual_cost)}
        Net ROI: {format_currency(net_roi)}
        ROI Percentage: {roi_percentage:.0f}%

        RECOMMENDATION
        --------------
        {'PROCEED with retention campaign. Expected positive ROI.' if net_roi > 0 else 'REVISE campaign parameters. Current projections show negative ROI.'}

        Focus retention efforts on the top {customers_to_contact:,} priority customers
        to maximize impact and ROI.
        """

        st.download_button(
            label="Download Executive Summary (TXT)",
            data=summary_report,
            file_name="churn_analysis_summary.txt",
            mime="text/plain",
            use_container_width=True
        )


# Main app logic
def main():
    """Main application logic with sidebar navigation"""

    # Sidebar
    st.sidebar.title("Navigation")
    st.sidebar.markdown("---")

    # Page selection
    page = st.sidebar.radio(
        "Go to",
        [
            "Home",
            "Predict Churn",
            "Data Analytics",
            "Model Performance",
            "Batch Predictions"
        ],
        label_visibility="collapsed"
    )

    st.sidebar.markdown("---")

    # Info section
    st.sidebar.markdown("### About")
    st.sidebar.info("""
    This system uses machine learning to predict customer churn and provide actionable retention strategies.

    **Covers 5 Course Outcomes:**
    - CO1: Heuristic Search
    - CO2: Preprocessing
    - CO3: Supervised Learning
    - CO4: Unsupervised Learning
    - CO5: Neural Networks
    """)

    st.sidebar.markdown("---")

    # Dataset info
    try:
        results = load_results()
        if results:
            st.sidebar.markdown("### Dataset Info")
            st.sidebar.metric("Total Samples", f"{results['dataset_info']['total_samples']:,}")
            st.sidebar.metric("Churn Rate", f"{results['dataset_info']['churn_rate']:.2f}%")
            st.sidebar.metric("Features", results['dataset_info']['features'])
    except:
        pass

    # Route to appropriate page
    if page == "Home":
        page_home()
    elif page == "Predict Churn":
        page_predict()
    elif page == "Data Analytics":
        page_analytics()
    elif page == "Model Performance":
        page_model_performance()
    elif page == "Batch Predictions":
        page_batch_predictions()


if __name__ == "__main__":
    main()
